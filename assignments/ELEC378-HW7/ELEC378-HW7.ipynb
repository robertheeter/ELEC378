{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee42dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBERT HEETER\n",
    "# ELEC 378 Machine Learning\n",
    "# 3 March 2023\n",
    "\n",
    "# PROBLEM SET 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92710072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# PART A\n",
    "\n",
    "df_train = pd.read_csv('train.csv')\n",
    "train_nums = df_train.select_dtypes(include='number') # select only numerical data types\n",
    "\n",
    "data = train_nums.values\n",
    "X = data[:,~np.isnan(data).any(axis=0)]\n",
    "y = X[:,-1] # final column of the data set is the actual sale price\n",
    "X = np.delete(X,-1,1)\n",
    "\n",
    "X = X[:,1:] # remove first column of data set since sklearn centers the data with fit_intercept\n",
    "\n",
    "print(f'rank(X) = {np.linalg.matrix_rank(X)}')\n",
    "print(f'\\ncondition(X) = {np.linalg.cond(X)}')\n",
    "print(f'\\ndiagonal matrix of singular values of X from SVD:\\n{np.linalg.svd(X)[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9685cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART B\n",
    "\n",
    "# ridge regression\n",
    "errors = []\n",
    "alphas = np.arange(1,10000,10)\n",
    "for a in alphas:\n",
    "    clf = Ridge(alpha=a, fit_intercept=True).fit(X, y)\n",
    "    y_approx = clf.predict(X)\n",
    "    errors.append(np.mean((np.abs(y-y_approx))/y)*100)\n",
    "plt.figure(0)\n",
    "plt.plot(alphas,errors)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('error (%)')\n",
    "plt.title('Error of ridge regression predicted sale price')\n",
    "plt.show()\n",
    "print(f'\\nminimum ridge regression error:\\nerror (%) = {errors[(np.argsort(errors))[0]]}\\nalpha = {alphas[(np.argsort(errors))[0]]}')\n",
    "print(f'\\nridge regression predicted sale prices ($):\\n{y_approx}')\n",
    "\n",
    "\n",
    "# unregularized linear regression (from HW #6)\n",
    "X_m = np.hstack((np.ones([len(X),1]),X)) # append column of 1's to data matrix to account for intercept (center of data)\n",
    "psuedo_X = np.linalg.pinv(X_m) # compute Moore-Penrose pseudoinverse, pseudo_X = (X^T*X)^-1 * X^T\n",
    "w = np.matmul(psuedo_X, y) # find optimal weightings, w = pseudo_X*y\n",
    "y_approx = np.matmul(X_m, w) # find calculated y (sale price) from regression, y_approx = X*w (+ error)\n",
    "error = np.mean((np.abs(y-y_approx))/y)*100\n",
    "print(f'\\nunregularized linear regression error = {error}%')\n",
    "print(f'\\nunregularized linear regression predicted sale prices ($):\\n{y_approx}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453a6d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART C\n",
    "\n",
    "# lasso regression\n",
    "errors = []\n",
    "alphas = np.arange(1,10000,10)\n",
    "for a in alphas:\n",
    "    clf = Lasso(alpha=a, fit_intercept=True).fit(X, y)\n",
    "    y_approx = clf.predict(X)\n",
    "    errors.append(np.mean((np.abs(y-y_approx))/y)*100)\n",
    "plt.figure(1)\n",
    "plt.plot(alphas,errors)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('error (%)')\n",
    "plt.title('Error of lasso regression predicted sale price')\n",
    "plt.show()\n",
    "print(f'\\nminimum lasso regression error:\\nerror (%) = {errors[(np.argsort(errors))[0]]}\\nalpha = {alphas[(np.argsort(errors))[0]]}')\n",
    "print(f'\\nlasso regression predicted sale prices ($):\\n{y_approx}')\n",
    "\n",
    "# unregularized linear regression (from HW #6)\n",
    "X_m = np.hstack((np.ones([len(X),1]),X)) # append column of 1's to data matrix to account for intercept (center of data)\n",
    "psuedo_X = np.linalg.pinv(X_m) # compute Moore-Penrose pseudoinverse, pseudo_X = (X^T*X)^-1 * X^T\n",
    "w = np.matmul(psuedo_X, y) # find optimal weightings, w = pseudo_X*y\n",
    "y_approx = np.matmul(X_m, w) # find calculated y (sale price) from regression, y_approx = X*w (+ error)\n",
    "error = np.mean((np.abs(y-y_approx))/y)*100\n",
    "print(f'\\nunregularized linear regression error = {error}%')\n",
    "print(f'\\nunregularized linear regression predicted sale prices ($):\\n{y_approx}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85912f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART D\n",
    "\n",
    "# assess feature weights between methodologies\n",
    "\n",
    "clf = Ridge(alpha=642, fit_intercept=True).fit(X, y) # ideal ridge regression based on part B (alpha = 642)\n",
    "w_ridge = clf.coef_\n",
    "print(f'\\nridge regression feature weights (w):\\n{w_ridge}')\n",
    "\n",
    "clf = Lasso(alpha=1268, fit_intercept=True).fit(X, y) # ideal lasso regression based on part C (alpha = 1268)\n",
    "w_lasso = clf.coef_\n",
    "print(f'\\nlasso regression feature weights (w):\\n{w_lasso}')\n",
    "\n",
    "# unregularized linear regression (from HW #6)\n",
    "X_m = np.hstack((np.ones([len(X),1]),X)) # append column of 1's to data matrix to account for intercept (center of data)\n",
    "psuedo_X = np.linalg.pinv(X_m) # compute Moore-Penrose pseudoinverse, pseudo_X = (X^T*X)^-1 * X^T\n",
    "w_unreg = np.matmul(psuedo_X, y) # find optimal weightings, w = pseudo_X*y\n",
    "print(f'\\nunregularized linear regression feature weights (w):\\n{w_unreg}')\n",
    "\n",
    "\n",
    "# assess feature importance ranking between methodologies\n",
    "\n",
    "labels = train_nums.columns[~np.isnan(data).any(axis=0)] # remove all columns that were ignored for linear regression\n",
    "labels_ridge = labels[1:-1] # remove first and last column of indices from labels\n",
    "w_i_ridge = np.flip(np.argsort(abs(w_ridge)))\n",
    "print('\\nmost to least impactful features (ridge regression):')\n",
    "print(labels_ridge[w_i_ridge])\n",
    "\n",
    "labels_lasso = labels[1:-1] # remove first and last column of indices from labels\n",
    "w_i_lasso = np.flip(np.argsort(abs(w_lasso)))\n",
    "print('\\nmost to least impactful features (lasso regression):')\n",
    "print(labels_lasso[w_i_lasso])\n",
    "\n",
    "labels_unreg = labels[1:-1] # remove first and last column of indices from labels\n",
    "w_i_unreg = np.flip(np.argsort(abs(w_unreg[1:]))) # ignore intercept (column 0) for weightings\n",
    "print('\\nmost to least impactful features (unregularized linear regression):')\n",
    "print(labels_unreg[w_i_unreg])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b17fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 4\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.stats import mode\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# PART A\n",
    "\n",
    "# load digits\n",
    "digits = load_digits()\n",
    "\n",
    "# split digits into training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2, random_state=0)\n",
    "\n",
    "# example image\n",
    "index = 100\n",
    "plt.imshow(X_train[index].reshape((8,8)))\n",
    "plt.show()\n",
    "print(f'Digit: {y_train[index]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddc8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_fit, y_fit, X_predict, n_neighbors=5, metric='euclidean'):\n",
    "    '''\n",
    "    inputs:\n",
    "        X_fit - 2D array containing all training data points\n",
    "        y_fit - 2D array containing all training data labels\n",
    "        X_predict - 2D array containing all data points to classify\n",
    "        n_neighbors - K\n",
    "        metric - see scipy.spatial.distance.cdist:\n",
    "                 https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html \n",
    "                 \n",
    "    returns: a 1D array of predicted labels for X_predict.\n",
    "    '''\n",
    "\n",
    "    # ensure that X_predict is two dimensional; this only matters if X_predict is one data point\n",
    "    if X_predict.ndim < 2:\n",
    "        X_predict = [X_predict]\n",
    "\n",
    "    # calculate distances\n",
    "    distances = cdist(X_predict, X_fit, metric)\n",
    "    \n",
    "    # find the data indices of least distance for each point; keep the closest n_neighbors data points for each point\n",
    "    closest = np.argsort(distances,axis=1)[:,0:n_neighbors]\n",
    "    \n",
    "    # find the label of the n_neighbors closest data points\n",
    "    closest_labels = y_fit[closest]\n",
    "    \n",
    "    # get the mode of each row and return the resulting 1D array of labels\n",
    "    y_predict = mode(closest_labels, axis=1, keepdims=True)[0][:,0]\n",
    "    \n",
    "    return y_predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc64c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART B,C,D\n",
    "\n",
    "metrics = ['euclidean', 'cityblock', 'chebyshev']\n",
    "plt.figure(2)\n",
    "\n",
    "for metric in metrics:\n",
    "    print(f'\\nDISTANCE METRIC = {metric}')\n",
    "    \n",
    "    errors = []\n",
    "    Ks = range(1,200)\n",
    "    \n",
    "    for K in Ks:\n",
    "\n",
    "        predictions = knn(X_fit=X_train, y_fit=y_train, X_predict=X_test, n_neighbors=K, metric=metric)\n",
    "        error = np.sum((y_test != predictions)*1)/len(predictions)*100\n",
    "        \n",
    "        errors.append(error)\n",
    "        \n",
    "        if K <= 20:\n",
    "            print(f'K = {K}; misclassification error: {error}%')\n",
    "    \n",
    "    plt.plot(Ks,errors)\n",
    "\n",
    "plt.legend(metrics)\n",
    "plt.xlabel('K (number of neighbors)')\n",
    "plt.ylabel('misclassification error (%)')\n",
    "plt.title('Error of K-nearest neighbors digit classification')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee3eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
