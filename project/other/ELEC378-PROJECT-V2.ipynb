{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ee42dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROBERT HEETER\n",
    "# ELEC 378 Machine Learning\n",
    "# 14 April 2023\n",
    "\n",
    "# PROJECT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91b0118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (window.IPython && IPython.notebook.kernel) IPython.notebook.kernel.execute('jovian.utils.jupyter.get_notebook_name_saved = lambda: \"' + IPython.notebook.notebook_name + '\"')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "# from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import jovian\n",
    "\n",
    "from IPython.display import Audio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da41dc0",
   "metadata": {},
   "source": [
    "## FUNCTIONS TO CONVERT AUDIO FILE TO SPECTROGRAM\n",
    "* from: https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce0f96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an audio file; return the signal as a tensor and the sample rate\n",
    "\n",
    "class AudioUtil():\n",
    "    def open(audio_file):\n",
    "        sig, sr = torchaudio.load(audio_file)\n",
    "        return (sig, sr)\n",
    "\n",
    "    def rechannel(aud, new_channel):\n",
    "        sig, sr = aud\n",
    "        \n",
    "        if (sig.shape[0] == new_channel):\n",
    "            # nothing to do\n",
    "            return aud\n",
    "\n",
    "        if (new_channel == 1):\n",
    "            # convert from stereo to mono by selecting only the first channel\n",
    "            resig = sig[:1, :]\n",
    "        else:\n",
    "            # convert from mono to stereo by duplicating the first channel\n",
    "            resig = torch.cat([sig, sig])\n",
    "\n",
    "        return ((resig, sr))\n",
    "\n",
    "    def resample(aud, newsr):\n",
    "        sig, sr = aud\n",
    "\n",
    "        if (sr == newsr):\n",
    "            # nothing to do\n",
    "            return aud\n",
    "\n",
    "        num_channels = sig.shape[0]\n",
    "        # resample first channel\n",
    "        resig = torchaudio.transforms.Resample(sr, newsr)(sig[:1, :])\n",
    "        \n",
    "        if (num_channels > 1):\n",
    "            # resample the second channel and merge both channels\n",
    "            retwo = torchaudio.transforms.Resample(sr, newsr)(sig[1:, :])\n",
    "            resig = torch.cat([resig, retwo])\n",
    "\n",
    "        return ((resig, newsr))\n",
    "\n",
    "    def pad_trunc(aud, max_ms):\n",
    "        sig, sr = aud\n",
    "        num_rows, sig_len = sig.shape\n",
    "        max_len = sr//1000 * max_ms\n",
    "\n",
    "        if (sig_len > max_len):\n",
    "            # truncate the signal to the given length\n",
    "            sig = sig[:, :max_len]\n",
    "\n",
    "        elif (sig_len < max_len):\n",
    "            # length of padding to add at the beginning and end of the signal\n",
    "            pad_begin_len = random.randint(0, max_len - sig_len)\n",
    "            pad_end_len = max_len - sig_len - pad_begin_len\n",
    "\n",
    "            # pad with 0s\n",
    "            pad_begin = torch.zeros((num_rows, pad_begin_len))\n",
    "            pad_end = torch.zeros((num_rows, pad_end_len))\n",
    "\n",
    "            sig = torch.cat((pad_begin, sig, pad_end), 1)\n",
    "\n",
    "        return (sig, sr)\n",
    "\n",
    "    def time_shift(aud, shift_limit):\n",
    "        sig, sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        \n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    def spectro_gram(aud, n_mels=64, n_fft=1024, hop_len=None):\n",
    "        sig, sr = aud\n",
    "        top_db = 80\n",
    "\n",
    "        # spec has shape [channel, n_mels, time], where channel is mono, stereo, etc.\n",
    "        spec = transforms.MelSpectrogram(\n",
    "            sr, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(sig)\n",
    "\n",
    "        # convert to decibels\n",
    "        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n",
    "        \n",
    "        return (spec)\n",
    "\n",
    "    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(\n",
    "                freq_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(\n",
    "                time_mask_param)(aug_spec, mask_value)\n",
    "\n",
    "        return aug_spec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe0d47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "    def __init__(self, df, data_path):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.duration = 4000\n",
    "        self.sr = 44100 # sampling rate\n",
    "        self.channel = 2\n",
    "        self.shift_pct = 0.4\n",
    "\n",
    "    # number of items in dataset\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    # get i'th item in dataset\n",
    "    def __getitem__(self, idx):\n",
    "        # absolute file path of the audio file; concatenate the audio directory with the relative path\n",
    "        audio_file = self.data_path + self.df.loc[idx, 'relative_path']\n",
    "        # get the Class ID\n",
    "        class_id = self.df.loc[idx, 'classID']\n",
    "\n",
    "        aud = AudioUtil.open(audio_file)\n",
    "        \n",
    "        # make all sounds have the same number of channels and same sample rate\n",
    "        reaud = AudioUtil.resample(aud, self.sr)\n",
    "        rechan = AudioUtil.rechannel(reaud, self.channel)\n",
    "\n",
    "        dur_aud = AudioUtil.pad_trunc(rechan, self.duration)\n",
    "        shift_aud = AudioUtil.time_shift(dur_aud, self.shift_pct)\n",
    "        sgram = AudioUtil.spectro_gram(\n",
    "            shift_aud, n_mels=64, n_fft=1024, hop_len=None)\n",
    "        aug_sgram = AudioUtil.spectro_augment(\n",
    "            sgram, max_mask_pct=0.1, n_freq_masks=2, n_time_masks=2)\n",
    "\n",
    "        return aug_sgram, class_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e70d92",
   "metadata": {},
   "source": [
    "## IMPORT & PROCESS TRAINING AND TRIAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb3651a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train dataset\n",
    "\n",
    "train_data = np.empty((1125, 2), dtype=object)\n",
    "count = 0\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(),'elec-378-sp2023-speech-emotion-classification/data/data')\n",
    "\n",
    "for filename in os.listdir(train_dir):\n",
    "    f = os.path.join(train_dir, filename)\n",
    "\n",
    "    # check if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        name = filename[:len(filename)-7]\n",
    "        train_data[count][0] = \"/\"+filename\n",
    "        train_data[count][1] = name\n",
    "        count += 1\n",
    "\n",
    "train_df = pd.DataFrame(train_data)\n",
    "train_df.rename(columns={0: \"relative_path\", 1: \"classID\"}, inplace=True)\n",
    "\n",
    "train_ds = SoundDS(train_df, train_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6735fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FOR TESTING; DELETE FOR FINAL KAGGLE SUBMISSION\n",
    "# # splits 85/15 train_ds into train_ds and trial_ds\n",
    "\n",
    "# train_ds, trial_ds = random_split(train_ds, [round(0.85*len(train_ds)),round(0.15*len(train_ds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14cdb507",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get audio tensor and label from train dataset; stack channels into one array\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in range(len(train_ds)):\n",
    "    audio_file = train_ds[i]\n",
    "    sample = audio_file[0]\n",
    "    label = audio_file[1]\n",
    "    \n",
    "    channel_1 = sample[0,:,:]\n",
    "    channel_2 = sample[1,:,:]\n",
    "    \n",
    "    audio_sample_combined = np.vstack((channel_1,channel_2))\n",
    "    \n",
    "    X_train.append(audio_sample_combined.flatten())\n",
    "    y_train.append(label)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "\n",
    "# map each label description to a label number\n",
    "\n",
    "description_to_number = {}\n",
    "for label in set(y_train):\n",
    "    description_to_number[label] = len(description_to_number)\n",
    "\n",
    "y_train = np.array([description_to_number[label] for label in y_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f156dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get trial dataset\n",
    "\n",
    "trial_data = np.empty((315, 2), dtype=object) \n",
    "count = 0\n",
    "\n",
    "trial_dir = os.path.join(os.getcwd(),'elec-378-sp2023-speech-emotion-classification/test/test')\n",
    "\n",
    "for filename in os.listdir(trial_dir):\n",
    "    f = os.path.join(trial_dir, filename)\n",
    "\n",
    "    # check if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        name = filename[:len(filename)-7]\n",
    "        trial_data[count][0] = \"/\"+filename\n",
    "        trial_data[count][1] = name\n",
    "        count += 1\n",
    "\n",
    "trial_df = pd.DataFrame(trial_data)\n",
    "trial_df.rename(columns={0: \"relative_path\", 1: \"classID\"}, inplace=True)\n",
    "\n",
    "trial_ds = SoundDS(trial_df, trial_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42c9bee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get audio tensor from trial dataset; stack channels into one array\n",
    "\n",
    "X_trial = []\n",
    "\n",
    "for i in range(len(trial_ds)):\n",
    "    audio_file = trial_ds[i]\n",
    "    sample = audio_file[0]\n",
    "    \n",
    "    channel_1 = sample[0,:,:]\n",
    "    channel_2 = sample[1,:,:]\n",
    "    \n",
    "    audio_sample_combined = np.vstack((channel_1,channel_2))\n",
    "\n",
    "    X_trial.append(audio_sample_combined.flatten())\n",
    "    \n",
    "X_trial = np.array(X_trial)\n",
    "\n",
    "# # get audio tensor and label from train dataset; stack channels into one array\n",
    "\n",
    "# X_trial = []\n",
    "# y_trial = []\n",
    "\n",
    "# for i in range(len(trial_ds)):\n",
    "#     audio_file = trial_ds[i]\n",
    "#     sample = audio_file[0]\n",
    "#     label = audio_file[1]\n",
    "    \n",
    "#     channel_1 = sample[0,:,:]\n",
    "#     channel_2 = sample[1,:,:]\n",
    "    \n",
    "#     audio_sample_combined = np.vstack((channel_1,channel_2))\n",
    "\n",
    "#     X_trial.append(audio_sample_combined)\n",
    "#     y_trial.append(label)\n",
    "    \n",
    "# X_trial = np.array(X_trial)\n",
    "\n",
    "# # map each label description to a label number\n",
    "\n",
    "# description_to_number = {}\n",
    "# for label in set(y_trial):\n",
    "#     description_to_number[label] = len(description_to_number)\n",
    "\n",
    "# y_trial = np.array([description_to_number[label] for label in y_trial])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab43d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X_train,y_train,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = X_train[10]\n",
    "label = y_train[10]\n",
    "plt.imshow(spectrogram.reshape((128,-1)), cmap = 'gray')\n",
    "print(\"Emotion #: \", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18025a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(X_train))\n",
    "print(np.shape(Y_train))\n",
    "a = np.where(Y_train==1)[0]\n",
    "print(a)\n",
    "for i in a:\n",
    "    spectrogram = X_train[i,:]\n",
    "#     label = y_train[10]\n",
    "    plt.imshow(spectrogram.reshape((128,-1)), cmap = 'gray')\n",
    "    plt.show()\n",
    "# print(\"Emotion #: \", label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71eb168c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ddeaab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rch/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg =  LogisticRegression(max_iter = 1000)\n",
    "logreg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a20063e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=logreg.predict(X_trial)\n",
    "y_pred_final = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d69b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting validation (testing) data \n",
    "test = np.empty((315, 2), dtype=object) \n",
    "count = 0\n",
    "directory = os.path.join(os.getcwd(),'elec-378-sp2023-speech-emotion-classification/test/test')\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        name = filename[:len(filename)-7]\n",
    "        test[count][0] = \"/\"+filename\n",
    "        test[count][1] = name\n",
    "        count += 1\n",
    "\n",
    "df = pd.DataFrame(test)\n",
    "df.rename(columns={0: \"relative_path\", 1: \"classID\"}, inplace=True)\n",
    "filename = df['relative_path'].to_list()\n",
    "val_ds = SoundDS(df, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f07c0fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'calm', 1: 'angry', 2: 'neutral', 3: 'fearful', 4: 'surprised', 5: 'sad', 6: 'disgust', 7: 'happy'}\n",
      "[6 1 5 4 6 7 6 7 5 1 3 1 4 5 7 6 6 7 5 4 5 5 0 4 1 7 0 4 7 7 2 7 7 3 1 7 6\n",
      " 3 2 7 2 6 1 7 3 0 4 0 3 5 1 3 5 4 6 5 1 7 3 1 0 2 6 1 6 1 1 5 1 7 2 7 0 6\n",
      " 1 1 0 5 0 2 3 7 6 1 2 5 4 2 5 6 2 2 1 5 6 5 1 6 5 6 3 5 7 4 3 2 5 3 3 5 0\n",
      " 1 4 4 3 0 3 4 7 3 6 5 0 3 6 7 5 1 6 7 6 3 5 5 7 1 5 0 1 5 3 3 4 1 4 4 6 3\n",
      " 4 4 7 5 5 1 7 4 2 6 3 0 0 3 7 7 4 4 7 1 7 5 0 7 2 7 7 1 4 1 0 4 1 5 6 1 5\n",
      " 5 2 7 3 1 4 0 5 5 4 7 4 0 6 4 7 3 6 5 4 7 2 7 5 3 6 5 1 2 1 6 3 4 3 3 1 2\n",
      " 4 5 5 7 0 1 0 0 4 5 0 3 2 5 0 3 6 7 3 5 6 6 2 7 4 5 6 7 2 4 6 5 5 1 7 6 5\n",
      " 4 7 3 3 5 6 3 7 7 1 6 7 3 3 5 0 7 7 5 4 4 5 3 1 4 6 6 6 6 2 1 5 5 0 3 0 1\n",
      " 3 7 6 1 5 0 0 4 0 7 6 0 3 7 1 4 5 3 2]\n",
      "{'calm': 0, 'angry': 1, 'neutral': 2, 'fearful': 3, 'surprised': 4, 'sad': 5, 'disgust': 6, 'happy': 7}\n"
     ]
    }
   ],
   "source": [
    "print(description_to_number)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "648bebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to kaggle upload\n",
    "number_to_label = {v: k for k, v in label_to_number.items()}\n",
    "# Map each number back to its corresponding label\n",
    "label = [label_to_number[number] for number in y_pred]\n",
    "clean_filename = [file[1:-4] for file in filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "44179c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "df = pd.DataFrame(list(zip(clean_filename, label)), columns=['filename', 'label'])\n",
    "df.to_csv(\"y_kaggle_LOGREG.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c3b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.sum(y_pred==Y_test)/len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a00f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data transformation\n",
    "import numpy as np\n",
    "# for visualizing the data\n",
    "import matplotlib.pyplot as plt\n",
    "# for opening the media file\n",
    "import scipy.io.wavfile as wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde88d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c9335f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd577aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = trial_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a48355c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a38f68c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a747d719",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b43fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(description_to_number)\n",
    "# emotions = ['surprised','sad','happy','fearful','calm','neutral','disgust','angry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a1722b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 256\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# other constants\n",
    "input_size = 128*344\n",
    "num_classes = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(X_train).float()\n",
    "targets = torch.from_numpy(y_train).long()\n",
    "# testinputs = torch.from_numpy(X_trial).float()\n",
    "# testtargets = torch.from_numpy(y_trial).long()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d0b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training validation & test dataset\n",
    "dataset = TensorDataset(inputs, targets)\n",
    "# testdataset = TensorDataset(testinputs, testtargets)\n",
    "\n",
    "# let's use 15% of our training dataset to validate our model\n",
    "num_rows = 956\n",
    "val_percent = 0.15\n",
    "val_size = int(num_rows * val_percent)\n",
    "train_size = num_rows - val_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7fca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2)\n",
    "# test_loader = DataLoader(testdataset, batch_size*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6f740",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, label = train_ds[3]\n",
    "plt.imshow(img.reshape((128,-1)), cmap = 'gray')\n",
    "plt.colorbar()\n",
    "print(\"Emotion: \", label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274772a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        xb = xb.reshape(-1, 44032)\n",
    "        out = self.linear(xb)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc.detach()}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))\n",
    "    \n",
    "model = MnistModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463e7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ebc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_loader):\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD):\n",
    "    history = []\n",
    "    optimizer = opt_func(model.parameters(), lr)\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f99711",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770ba657",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = fit(50, 1e-5, model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63abbd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing how our model performed after each epoch\n",
    "accuracies = [r['val_acc'] for r in history]\n",
    "plt.plot(accuracies, '-x')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('Accuracy vs. No. of epochs');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21825ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "train_ds [audio file #] [tensor 0 or label 1]\n",
    "\n",
    "3D tensor [num_channels, Mel freq_bands, time_steps]\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3996ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[1][0,:,:], cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb778dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa as lb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b17fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = os.path.join(os.getcwd(),'elec-378-sp2023-speech-emotion-classification/data/data')\n",
    "\n",
    "data_sr = 48000 # sampling rate of audio files is 48000\n",
    "\n",
    "# y_maxsize = 253053 # max size audio sample in data\n",
    "y_maxsize = 300000\n",
    "\n",
    "X = []\n",
    "for filename in os.listdir(data):\n",
    "    audiofile = os.path.join(data,filename)\n",
    "    \n",
    "    if os.path.isfile(audiofile):\n",
    "        y, sr = lb.load(audiofile,sr=data_sr)\n",
    "        \n",
    "        y = librosa.util.fix_length(y, size=y_maxsize)\n",
    "        X.append(y)\n",
    "        \n",
    "X = np.array(X)\n",
    "# print(np.shape(X))\n",
    "\n",
    "for y in X:\n",
    "    chroma_stft = lb.feature.chroma_stft(y=y, sr=data_sr)\n",
    "    chroma_cqt = lb.feature.chroma_cqt(y=y, sr=data_sr)\n",
    "    chroma_cens = lb.feature.chroma_cens(y=y, sr=data_sr)\n",
    "#     chroma_stft = lb.feature.chroma_stft(y=y, sr=data_sr)\n",
    "    mfcc = lb.feature.mfcc(y=y, sr=data_sr)\n",
    "    rms = lb.feature.rms(y=y)\n",
    "    \n",
    "    break\n",
    "# \n",
    "# print(chroma_stft)\n",
    "print(np.shape(chroma_stft))\n",
    "print(np.shape(chroma_cqt))\n",
    "print(np.shape(chroma_cens))\n",
    "print(np.shape(mfcc))\n",
    "print(np.shape(rms))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.inf)\n",
    "# print(chroma_stft.T)\n",
    "print(rms[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7823fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
