{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33c798f0",
   "metadata": {},
   "source": [
    "# **ELEC 378 FINAL PROJECT: SPEECH EMOTION CLASSIFICATION**\n",
    "* Team JARL\n",
    "* Jasmine Lee, Arielle Sanford, Robert Heeter, Lindsey Russ\n",
    "* ELEC 378: Machine Learning: Concepts & Techniques\n",
    "* Rice University\n",
    "\n",
    "* Submitted 2 May 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f49467",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc82de3b",
   "metadata": {},
   "source": [
    "## **Get training dataset and calculate MFCCs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab438b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(os.getcwd(),'elec-378-sp2023-speech-emotion-classification/data/data/')\n",
    "data = np.empty((1125, 2), dtype=object)\n",
    "\n",
    "emotion_to_id = {\n",
    "    \"angry\" : 0,\n",
    "    \"calm\" : 1,\n",
    "    \"disgust\" : 2,\n",
    "    \"fearful\" : 3,\n",
    "    \"happy\" : 4,\n",
    "    \"neutral\" : 5,\n",
    "    \"sad\" : 6,\n",
    "    \"surprised\" : 7\n",
    "}\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "\n",
    "    if os.path.isfile(f):\n",
    "        emotion = filename[:len(filename)-7]\n",
    "        data[i][0] = \"/\"+filename\n",
    "        data[i][1] = int(emotion_to_id[emotion])\n",
    "        i += 1\n",
    "\n",
    "def make_mfcc(file, n_mfcc):\n",
    "    sig, sr = librosa.load(file)\n",
    "    sig_mfcc = librosa.feature.mfcc(y=sig, sr=sr, n_mfcc=n_mfcc, S=None, htk=True)\n",
    "    sig_mfcc_avg = np.mean(sig_mfcc, axis=1)\n",
    "\n",
    "    return sig_mfcc_avg\n",
    "\n",
    "n_mfcc = 38\n",
    "\n",
    "X = np.empty((len(data), n_mfcc), dtype=float)\n",
    "y = np.empty((len(data)), dtype=int)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    file = directory + data[i][0]\n",
    "    X[i] = make_mfcc(file, n_mfcc=n_mfcc)\n",
    "    y[i] = data[i][1]\n",
    "    \n",
    "X_train = X\n",
    "y_train = y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e48a4d",
   "metadata": {},
   "source": [
    "## **Get testing dataset and calculate MFCCs (FOR KAGGLE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory = os.path.join(os.getcwd(),'elec-378-sp2023-speech-emotion-classification/test/test/')\n",
    "# data = np.empty((315, 2), dtype=object)\n",
    "\n",
    "# emotion_to_id = {\n",
    "#     \"angry\" : 0,\n",
    "#     \"calm\" : 1,\n",
    "#     \"disgust\" : 2,\n",
    "#     \"fearful\" : 3,\n",
    "#     \"happy\" : 4,\n",
    "#     \"neutral\" : 5,\n",
    "#     \"sad\" : 6,\n",
    "#     \"surprised\" : 7\n",
    "# }\n",
    "\n",
    "# i = 0\n",
    "# for filename in os.listdir(directory):\n",
    "#     f = os.path.join(directory, filename)\n",
    "\n",
    "#     if os.path.isfile(f):\n",
    "#         data[i][0] = \"/\"+filename\n",
    "#         i += 1\n",
    "\n",
    "# def make_mfcc(file, n_mfcc):\n",
    "#     sig, sr = librosa.load(file)\n",
    "#     sig_mfcc = librosa.feature.mfcc(y=sig, sr=sr, n_mfcc=n_mfcc, S=None, htk=True)\n",
    "#     sig_mfcc_avg = np.mean(sig_mfcc, axis=1)\n",
    "\n",
    "#     return sig_mfcc_avg\n",
    "\n",
    "# n_mfcc = 3\n",
    "\n",
    "# X = np.empty((len(data), n_mfcc), dtype=float)\n",
    "\n",
    "# for i in range(len(data)):\n",
    "#     file = directory + data[i][0]\n",
    "#     X[i] = make_mfcc(file, n_mfcc=n_mfcc)\n",
    "\n",
    "# X_test = X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b22cc",
   "metadata": {},
   "source": [
    "## **Split train/test data (NOT FOR KAGGLE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994b3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d01df",
   "metadata": {},
   "source": [
    "## **Support vector machine (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68199538",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = make_pipeline(RobustScaler(), SVC(C=20, tol=0.001))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"n_mfcc: {n_mfcc}, c: {c}, acc: {accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e87bf",
   "metadata": {},
   "source": [
    "## **Multilayer perceptron (MLP)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a191d75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_params = {'activation': 'relu', \n",
    "              'solver': 'lbfgs', \n",
    "              'hidden_layer_sizes': 1283, \n",
    "              'alpha': 0.3849485717707319, \n",
    "              'batch_size': 163, \n",
    "              'learning_rate': 'constant',\n",
    "              'max_iter':1000}\n",
    "\n",
    "clf = make_pipeline(RobustScaler(), MLPClassifier(**mlp_params))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"{accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f9818",
   "metadata": {},
   "source": [
    "## **Convolutional neural network (CNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48f7e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D, BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model2.add(layers.Conv2D(64, (4, 4), activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "# model2.add(layers.MaxPooling2D((2, 2)))# Hidden Layer 2\n",
    "# model2.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_regularizer=regularizers.l2(l=0.01)))\n",
    "# model2.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "# model3.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', input_shape=(96, 96, 3)))\n",
    "\n",
    "# random.seed(123) # Establish Consistency in resultsmodel4 = Sequential() # Instantiate the 4th Modelmodel4.add(layers.Conv2D(32, (3, 3), activation=’relu’, input_shape=(96, 96, 3)))\n",
    "# model4.add(layers.MaxPooling2D((2, 2)))\n",
    "# model4.add(Dropout(0.4))\n",
    "# model4.add(layers.Conv2D(64, (4, 4), activation='relu'))\n",
    "# model4.add(layers.MaxPooling2D((2, 2)))\n",
    "# model4.add(Dropout(0.4)) # Flattening- Convert 2D matrix to a 1D vector\n",
    "# model4.add(layers.Flatten())\n",
    "# model4.add(layers.Dense(512, activation = 'relu'))\n",
    "# model4.add(Dropout(0.2))\n",
    "# model4.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model5.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_constraint=unit_norm(), input_shape=(96, 96, 3)))\n",
    "\n",
    "\n",
    "# model.add(Conv1D(128, 16, padding='same', input_shape=(40,1)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Conv1D(128, 16, padding='same', input_shape=(40,1)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(Dropout(0.4))\n",
    "\n",
    "# model.add(MaxPooling1D(pool_size=8))\n",
    "\n",
    "# model.add(Conv1D(128, 16, padding='same', input_shape=(40,1)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Conv1D(128, 16, padding='same', input_shape=(40,1)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Conv1D(128, 16, padding='same', input_shape=(40,1)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # model.add(Conv1D(128, 16, padding='same', input_shape=(40,1)))\n",
    "# # model.add(BatchNormalization())\n",
    "# # model.add(Activation('relu'))\n",
    "\n",
    "# # model.add(MaxPooling1D(pool_size=5))\n",
    "\n",
    "# # model.add(Conv1D(128, 5, padding='same', input_shape=(40,1)))\n",
    "# # model.add(Activation('relu'))\n",
    "\n",
    "# # model.add(Conv1D(128, 5, padding='same', input_shape=(40,1)))\n",
    "# # model.add(Activation('relu'))\n",
    "\n",
    "# # model.add(Dropout(0.2))\n",
    "# model.add(Flatten())\n",
    "# # model.add(Dense(10, kernel_regularizer='l2', bias_regularizer='l2'))\n",
    "# model.add(Dense(8))\n",
    "# # model.add(Activation('softmax'))\n",
    "\n",
    "# opt = keras.optimizers.RMSprop(learning_rate=0.0001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "\n",
    "# model.add(Conv1D(128, 5, padding='same', input_shape=(40,1)))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(MaxPooling1D(pool_size=(8)))\n",
    "# model.add(Conv1D(128, 5, padding='same',))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('softmax'))\n",
    "# opt = keras.optimizers.RMSprop(learning_rate=0.0005, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.add(Conv1D(128, 5,padding='same', input_shape=(34,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.RMSprop(lr=0.00005, rho=0.9, epsilon=None, decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc09c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afa1aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_cnn = np.expand_dims(X_train, axis=2)\n",
    "X_test_cnn = np.expand_dims(X_test, axis=2)\n",
    "\n",
    "cnnhistory = model.fit(X_train_cnn, y_train, batch_size=4, epochs=60, validation_data=(X_test_cnn, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cnnhistory.history['accuracy'])\n",
    "plt.plot(cnnhistory.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca275fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test_cnn, y_test)\n",
    "print(f\"{accuracy*100}%\")\n",
    "\n",
    "y_pred = model.predict(X_test_cnn)\n",
    "print(np.shape(y_pred))\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "print(y_pred)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"{accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972ce411",
   "metadata": {},
   "source": [
    "## **Logistic regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54da715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = make_pipeline(RobustScaler(), LogisticRegression(tol=0.00001, max_iter=10000))\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ff0eb5",
   "metadata": {},
   "source": [
    "## **k-nearest neighbors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = make_pipeline(RobustScaler(), KNeighborsClassifier(n_neighbors = 8))\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbcf9d8",
   "metadata": {},
   "source": [
    "## **Other models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c0ba7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import glob \n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# import xgboost as xgb\n",
    "# import optuna\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b40a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(file_name):\n",
    "    \"\"\"Function Extracts Features from WAV file\"\"\"\n",
    "    X, sample_rate = librosa.load(file_name)\n",
    "    stft=np.abs(librosa.stft(X))\n",
    "    result=np.array([])\n",
    "    mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
    "    result=np.hstack((result, mfccs))\n",
    "    chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "    result=np.hstack((result, chroma))\n",
    "    mel=np.mean(librosa.feature.melspectrogram(y=X, sr=sample_rate).T,axis=0)\n",
    "    result=np.hstack((result, mel))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28e6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = {\n",
    "    \"angry\" : 0,\n",
    "    \"calm\" : 1,\n",
    "    \"disgust\" : 2,\n",
    "    \"fearful\" : 3,\n",
    "    \"happy\" : 4,\n",
    "    \"neutral\" : 5,\n",
    "    \"sad\" : 6,\n",
    "    \"surprised\" : 7\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddd371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(test_size=0.2):\n",
    "    x,y=[],[]\n",
    "    \n",
    "    directory = os.path.join(os.getcwd(),'elec-378-sp2023-speech-emotion-classification/data/data/')\n",
    "\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        file = os.path.join(directory, filename)\n",
    "\n",
    "        emotion=emotions[filename[:len(filename)-7]]\n",
    "        feature=extract_feature(file)\n",
    "        x.append(feature)\n",
    "        y.append(emotion)\n",
    "\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b81b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_data()\n",
    "print((X_train.shape[0], X_test.shape[0]))\n",
    "\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "print(np.shape(X_test))\n",
    "print(X_train[0])\n",
    "\n",
    "print(f'Features extracted: {X_train.shape[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac26413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f7316",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_params = {'activation': 'relu', \n",
    "              'solver': 'lbfgs', \n",
    "              'hidden_layer_sizes': 1283, \n",
    "              'alpha': 0.3849485717707319, \n",
    "              'batch_size': 163, \n",
    "              'learning_rate': 'constant',\n",
    "              'max_iter':1000}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eace5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_model = MLPClassifier(**mlp_params)\n",
    "# clf_model.fit(X_train, y_train)\n",
    "# y_pred = clf_model.predict(X_test)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "scaler = StandardScaler().fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = MLPClassifier(**mlp_params)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(f\"{accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff27b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "v4_params = {'estimators':[('mlp', models['mlp']), \n",
    "                          ('xgb', models['xgb'])], \n",
    "            'voting':'soft'}\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "scaler = StandardScaler().fit(X_test)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = make_pipeline(RobustScaler(), VotingClassifier(**v4_params))\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf_model.predict(X_test)\n",
    "\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(y_pred)\n",
    "print(f\"{accuracy*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2dc7b9",
   "metadata": {},
   "source": [
    "## **Exporting predictions (FOR KAGGLE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f35079",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_emotion = dict((v, k) for k, v in emotion_to_id.items())\n",
    "y_pred = [id_to_emotion[x] for x in y_pred]\n",
    "\n",
    "print(np.shape(y_pred))\n",
    "print(y_pred)\n",
    "\n",
    "import pandas as pd\n",
    "names = [x[1:len(x)-4] for x in data[:,0]]\n",
    "\n",
    "df = pd.DataFrame(list(zip(names, y_pred)), columns=['filename', 'label'])\n",
    "df.to_csv(\"y_kaggle_svm16.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
